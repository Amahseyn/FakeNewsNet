{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from transformers import RobertaTokenizer, RobertaForSequenceClassification\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Load datasets\n",
        "df_buzzfeed = pd.read_csv(\"/content/drive/MyDrive/buzzfeed_news_with_filenames.csv\")\n",
        "df_politifact = pd.read_csv(\"/content/drive/MyDrive/politifact_news_with_filenames.csv\")\n",
        "df = pd.concat([df_buzzfeed, df_politifact], ignore_index=True)\n",
        "df['label'] = df['label'].map({'fake': 0, 'real': 1})\n",
        "\n",
        "\n",
        "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
        "\n",
        "class RobertaDataset(Dataset):\n",
        "    def __init__(self, dataframe, tokenizer, max_length=512):\n",
        "        self.data = dataframe\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.data.iloc[idx]\n",
        "        text = str(row['title']) + \" \" + str(row['text'])\n",
        "        encoding = self.tokenizer(\n",
        "            text,\n",
        "            truncation=True,\n",
        "            max_length=self.max_length,\n",
        "            padding='max_length',\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "        input_ids = encoding['input_ids'].squeeze(0)\n",
        "        attention_mask = encoding['attention_mask'].squeeze(0)\n",
        "        label = torch.tensor(row['label'], dtype=torch.long)\n",
        "        return input_ids, attention_mask, label\n",
        "\n",
        "\n",
        "train_df, val_df = train_test_split(df, test_size=0.2, stratify=df['label'], random_state=42)\n",
        "val_df, test_df = train_test_split(val_df, test_size=0.5, stratify=val_df['label'], random_state=42)\n",
        "\n",
        "train_dataset = RobertaDataset(train_df, tokenizer)\n",
        "val_dataset = RobertaDataset(val_df, tokenizer)\n",
        "test_dataset = RobertaDataset(test_df, tokenizer)\n",
        "\n",
        "def collate_fn(batch):\n",
        "    input_ids = torch.stack([item[0] for item in batch])\n",
        "    attention_masks = torch.stack([item[1] for item in batch])\n",
        "    labels = torch.stack([item[2] for item in batch])\n",
        "    return input_ids, attention_masks, labels\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, collate_fn=collate_fn)\n",
        "val_loader = DataLoader(val_dataset, batch_size=16, collate_fn=collate_fn)\n",
        "test_loader = DataLoader(test_dataset, batch_size=16, collate_fn=collate_fn)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = RobertaForSequenceClassification.from_pretrained('roberta-base', num_labels=2)\n",
        "model.to(device)\n",
        "\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n",
        "\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "def train_model(model, train_loader, val_loader, epochs=5):\n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "        total_loss = 0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        for input_ids, attention_mask, labels in tqdm(train_loader):\n",
        "            input_ids = input_ids.to(device)\n",
        "            attention_mask = attention_mask.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(input_ids, attention_mask=attention_mask)\n",
        "            logits = outputs.logits\n",
        "            loss = criterion(logits, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item() * input_ids.size(0)\n",
        "            preds = torch.argmax(logits, dim=1)\n",
        "            correct += (preds == labels).sum().item()\n",
        "            total += input_ids.size(0)\n",
        "            train_acc = correct / total\n",
        "            val_acc, val_prec, val_rec, val_f1 = evaluate(model, val_loader)\n",
        "\n",
        "        print(f\"Epoch {epoch+1} | \"\n",
        "                  f\"Train Loss: {total_loss/total:.4f} | \"\n",
        "                  f\"Train Acc: {train_acc:.4f} | \"\n",
        "                  f\"Val Acc: {val_acc:.4f} | \"\n",
        "\n",
        "                  f\"Rec: {val_rec:.4f} | \"\n",
        "                  f\"F1: {val_f1:.4f}\")\n",
        "\n",
        "def evaluate(model, loader):\n",
        "    model.eval()\n",
        "    preds = []\n",
        "    labels_all = []\n",
        "    with torch.no_grad():\n",
        "        for input_ids, attention_mask, labels in loader:\n",
        "            input_ids = input_ids.to(device)\n",
        "            attention_mask = attention_mask.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            outputs = model(input_ids, attention_mask=attention_mask)\n",
        "            logits = outputs.logits\n",
        "            batch_preds = torch.argmax(logits, dim=1)\n",
        "            preds.extend(batch_preds.cpu().numpy())\n",
        "            labels_all.extend(labels.cpu().numpy())\n",
        "    model.train()\n",
        "\n",
        "    acc = accuracy_score(labels_all, preds)\n",
        "    precision = precision_score(labels_all, preds)\n",
        "    recall = recall_score(labels_all, preds)\n",
        "    f1 = f1_score(labels_all, preds)\n",
        "    return acc, precision, recall, f1\n",
        "\n",
        "train_model(model, train_loader, val_loader, epochs=10)\n",
        "\n",
        "test_acc, test_prec, test_rec, test_f1 = evaluate(model, test_loader)\n",
        "print(f\"Test Accuracy: {test_acc:.4f}\")\n",
        "print(f\"Test Precision: {test_prec:.4f}\")\n",
        "print(f\"Test Recall: {test_rec:.4f}\")\n",
        "print(f\"Test F1 Score: {test_f1:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fy5Of-fbg5lK",
        "outputId": "34b180b1-7a2c-401c-f13b-85ab5e7e90ca"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "  0%|          | 0/22 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "  5%|▍         | 1/22 [00:04<01:25,  4.07s/it]/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "100%|██████████| 22/22 [01:06<00:00,  3.01s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 | Train Loss: 0.6720 | Train Acc: 0.5312 | Val Acc: 0.8095 | Rec: 0.6190 | F1: 0.7647\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 22/22 [01:06<00:00,  3.04s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2 | Train Loss: 0.4971 | Train Acc: 0.7982 | Val Acc: 0.7619 | Rec: 0.7619 | F1: 0.7619\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 22/22 [01:04<00:00,  2.91s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3 | Train Loss: 0.3592 | Train Acc: 0.8398 | Val Acc: 0.8571 | Rec: 0.7143 | F1: 0.8333\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 22/22 [01:03<00:00,  2.90s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4 | Train Loss: 0.3710 | Train Acc: 0.8576 | Val Acc: 0.9048 | Rec: 0.8095 | F1: 0.8947\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 22/22 [01:03<00:00,  2.90s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5 | Train Loss: 0.2174 | Train Acc: 0.9080 | Val Acc: 0.8810 | Rec: 0.7619 | F1: 0.8649\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 22/22 [01:04<00:00,  2.91s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6 | Train Loss: 0.1969 | Train Acc: 0.9139 | Val Acc: 0.7381 | Rec: 1.0000 | F1: 0.7925\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 22/22 [01:03<00:00,  2.90s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 7 | Train Loss: 0.2797 | Train Acc: 0.8694 | Val Acc: 0.6905 | Rec: 0.9048 | F1: 0.7451\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 22/22 [01:03<00:00,  2.91s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 8 | Train Loss: 0.1066 | Train Acc: 0.9763 | Val Acc: 0.7619 | Rec: 0.9524 | F1: 0.8000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 22/22 [01:03<00:00,  2.91s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 9 | Train Loss: 0.1187 | Train Acc: 0.9585 | Val Acc: 0.8571 | Rec: 0.9048 | F1: 0.8636\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 22/22 [01:03<00:00,  2.90s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10 | Train Loss: 0.0332 | Train Acc: 0.9881 | Val Acc: 0.8095 | Rec: 0.9048 | F1: 0.8261\n",
            "Test Accuracy: 0.8372\n",
            "Test Precision: 0.7917\n",
            "Test Recall: 0.9048\n",
            "Test F1 Score: 0.8444\n"
          ]
        }
      ]
    }
  ]
}